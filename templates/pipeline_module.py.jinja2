import pandas as pd
from sqlalchemy.orm import Session
from sqlalchemy.dialects.postgresql import insert as pg_insert
from db.utils import load_csv, get_csv_path_for
from db.database import run_with_session
from db.models import {{ model_name }}

CSV = [
    {% for csv_file in csv_files %}
    get_csv_path_for("{{ csv_file }}"),
    {% endfor %}
]

table_name = "{{ table_name }}"

def load():
    return load_csv(CSV)

def clean(df: pd.DataFrame) -> pd.DataFrame:
    """Remove rows with missing or malformed data"""
    if df.empty:
        print(f"No {table_name} data to clean.")
        return df

    print(f"\nCleaning {table_name} data...")
    initial_count = len(df)

    {% if csv_files|length > 1 %}
    # Deduplicate unified data
    df = df.drop_duplicates(keep='first')
    {% endif %}

    {% if specs.similarities %}
    # Remove rows with unwanted similarity values  
    {% for similarity in specs.similarities %}
    {% for bad_value in similarity['values'][1:] %}
    mask = (df['{{ specs.pk_column }}'].astype(str) == '{{ similarity['pk_value'] }}') & (df['{{ similarity['column'] }}'] == '{{ bad_value }}')
    df = df[~mask]
    {% endfor %}
    {% endfor %}
    {% endif %}

    {% if specs.conflicts %}
    # Resolve conflicts by updating PK values to synthetic PKs
    {% for conflict in specs.conflicts %}
    {% for conflict_value, new_pk in conflict['value_pk_mapping'].items() %}
    mask = (df['{{ specs.pk_column }}'].astype(str) == '{{ conflict['pk_value'] }}') & (df['{{ conflict['column'] }}'] == "{{ conflict_value }}")
    df.loc[mask, '{{ specs.pk_column }}'] = {{ new_pk }}
    {% endfor %}
    {% endfor %}
    {% endif %}

    {% if specs.get('conflict_pk_mappings') %}
    # Update foreign key values based on core table conflict resolutions
    {% for core_table, pk_mappings in specs.conflict_pk_mappings.items() %}
    {% for original_pk, mapping_info in pk_mappings.items() %}
    {% for conflict_value, new_pk in mapping_info.value_mapping.items() %}
    # Find rows with this descriptive value and update the FK column
    mask = (df['{{ mapping_info.column }}'] == '{{ conflict_value }}')
    if mask.any():
        fk_column = '{{ mapping_info.fk_column_name }}'
        if fk_column in df.columns:
            df.loc[mask, fk_column] = {{ new_pk }}
            print(f"Updated {mask.sum()} rows: {fk_column} = {{ new_pk }} for '{{ conflict_value }}'")
    {% endfor %}
    {% endfor %}
    {% endfor %}
    {% endif %}

    {% if specs.exclude_columns %}
    # Remove redundant columns that can be looked up via foreign keys
    columns_to_drop = [col for col in {{ specs.exclude_columns }} if col in df.columns]
    if columns_to_drop:
        df = df.drop(columns=columns_to_drop)
        print(f"Dropped redundant columns: {columns_to_drop}")
    {% endif %}

    {% for column in csv_analysis.column_analysis %}
    # DEBUG: Column {{ column.csv_column_name }} - Excluded: {{ column.csv_column_name in specs.exclude_columns if specs.exclude_columns else False }}
    {% if specs.exclude_columns and column.csv_column_name in specs.exclude_columns %}
    # SKIPPING excluded column: {{ column.csv_column_name }} {{ specs.exclude_columns}}
    {% else %}
    # Clean {{ column.csv_column_name }} -> {{ column.sql_column_name }}
    df['{{ column.csv_column_name }}'] = df['{{ column.csv_column_name }}'].astype(str).str.strip().str.replace("'", "")
    {% if column.inferred_sql_type == 'Integer' %}
    df['{{ column.csv_column_name }}'] = pd.to_numeric(df['{{ column.csv_column_name }}'], errors='coerce').astype('Int64')
    {% elif column.inferred_sql_type == 'Float' %}
    df['{{ column.csv_column_name }}'] = pd.to_numeric(df['{{ column.csv_column_name }}'], errors='coerce')
    {% endif %}
    {% endif %}
    {% endfor %}

    final_count = len(df)
    print(f"Validated {table_name}: {initial_count} → {final_count} rows")
    return df

def insert(df: pd.DataFrame, session: Session):
    """Insert into the database."""
    if df.empty:
        print(f"No {table_name} data to insert.")
        return

    print(f"\nPreparing bulk {table_name} insert...")

    try:
        # Determine chunk size based on row count
        {% if csv_analysis.row_count > 10000 %}
        CHUNK_SIZE = 5000  # Large dataset - use chunking
        total_inserted = 0
        
        for i in range(0, len(df), CHUNK_SIZE):
            chunk = df.iloc[i:i + CHUNK_SIZE]
            records = []
            for _, row in chunk.iterrows():
                records.append({
                    {% for column in csv_analysis.column_analysis %}
                    '{{ column.sql_column_name }}': row['{{ column.csv_column_name }}'],
                    {% endfor %}
                })
            
            stmt = pg_insert({{ model_name }}).values(records)
            stmt = stmt.on_conflict_do_nothing()
            session.execute(stmt)
            session.commit()
            
            total_inserted += len(records)
            print(f"Processed chunk {i//CHUNK_SIZE + 1}: {total_inserted:,} rows inserted")
        
        print(f"✅ {table_name} insert complete: {total_inserted:,} total rows")
        {% else %}
        # Small dataset - single batch insert
        records = []
        for _, row in df.iterrows():
            records.append({
                {% for column in csv_analysis.column_analysis %}
                '{{ column.sql_column_name }}': row['{{ column.csv_column_name }}'],
                {% endfor %}
            })
        
        stmt = pg_insert({{ model_name }}).values(records)
        stmt = stmt.on_conflict_do_nothing()
        session.execute(stmt)
        session.commit()
        print(f"✅ Inserted {len(df)} {table_name} rows")
        {% endif %}
    except Exception as e:
        print(f"Error inserting {table_name}: {e}")
        session.rollback()

def run(db):
    df = load()
    df = clean(df)
    insert(df, db)

if __name__ == "__main__":
    run_with_session(run)