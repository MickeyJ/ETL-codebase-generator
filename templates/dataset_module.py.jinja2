# templates/dataset_module.py.jinja2
import pandas as pd
from sqlalchemy.orm import Session
from sqlalchemy.dialects.postgresql import insert as pg_insert
from {{ project_name }}.src.db.utils import load_csv, get_csv_path_for, generate_numeric_id
from {{ project_name }}.src.db.database import run_with_session
from .{{ table_name }}_model import {{ model_name }}

# Dataset CSV file
CSV_PATH = get_csv_path_for("{{ csv_file }}")

table_name = "{{ table_name }}"
CHUNK_SIZE = 10000  # Process in chunks for large datasets


def load():
    """Load the dataset CSV file"""
    return load_csv(CSV_PATH)


def clean(df: pd.DataFrame) -> pd.DataFrame:
    """Clean and prepare dataset data"""
    if df.empty:
        print(f"No {table_name} data to clean.")
        return df

    print(f"\nCleaning {table_name} data...")
    initial_count = len(df)
    
    # Basic column cleanup
    {% for column in column_analysis %}
    {% if specs.exclude_columns | length and column.csv_column_name not in specs.exclude_columns %}
    df['{{ column.csv_column_name }}'] = df['{{ column.csv_column_name }}'].astype(str).str.strip().str.replace("'", "")
    {% if column.inferred_sql_type == 'Integer' %}
    df['{{ column.csv_column_name }}'] = pd.to_numeric(df['{{ column.csv_column_name }}'], errors='coerce')
    {% elif column.inferred_sql_type == 'Float' %}
    df['{{ column.csv_column_name }}'] = pd.to_numeric(df['{{ column.csv_column_name }}'], errors='coerce')
    {% endif %}
    {% endif %}
    {% endfor %}
    
    {% if specs.foreign_keys %}
    # Generate hash IDs for foreign key columns
    dataset_name = "{{ table_name }}"  # This dataset's name
    
    {% for fk in specs.foreign_keys %}
    # Generate hash IDs for {{ fk.actual_column_name }}
    df['{{ fk.actual_column_name }}_id'] = df['{{ fk.actual_column_name }}'].apply(
        lambda val: generate_numeric_id(
            {'{{ fk.lookup_pk_column }}': str(val), 'source_dataset': dataset_name},
            ['{{ fk.lookup_pk_column }}', 'source_dataset']
        ) if pd.notna(val) and str(val).strip() else None
    )
    {% endfor %}
    {% endif %}
    
    {% if specs.exclude_columns %}
    # Remove redundant columns that can be looked up via foreign keys
    columns_to_drop = [col for col in {{ specs.exclude_columns | tojson }} if col in df.columns]
    if columns_to_drop:
        df = df.drop(columns=columns_to_drop)
        print(f"  Dropped redundant columns: {columns_to_drop}")
    {% endif %}
    
    # Remove complete duplicates
    df = df.drop_duplicates()
    
    final_count = len(df)
    print(f"  Cleaned: {initial_count} → {final_count} rows")
    return df


def insert(df: pd.DataFrame, session: Session):
    """Insert dataset data with chunking for large files"""
    if df.empty:
        print(f"No {table_name} data to insert.")
        return
    
    print(f"\nInserting {table_name} data ({len(df):,} rows)...")
    
    total_rows = len(df)
    total_inserted = 0
    
    # Process in chunks
    for chunk_idx, start_idx in enumerate(range(0, total_rows, CHUNK_SIZE)):
        end_idx = min(start_idx + CHUNK_SIZE, total_rows)
        chunk_df = df.iloc[start_idx:end_idx]
        
        records = []
        for _, row in chunk_df.iterrows():
            record = {}
             {% if specs.foreign_keys %}
            # Add the hash ID columns
            {% for fk in specs.foreign_keys %}
            record['{{ fk.column_name }}_id'] = row['{{ fk.actual_column_name }}_id']
            {% endfor %}
            {% endif %}
            {% for column in column_analysis %}
            {% if column.csv_column_name not in specs.exclude_columns %}
            record['{{ column.sql_column_name }}'] = row['{{ column.csv_column_name }}']
            {% endif %}
            {% endfor %}
            records.append(record)
        
        if records:
            try:
                stmt = pg_insert({{ model_name }}).values(records)
                stmt = stmt.on_conflict_do_nothing()
                result = session.execute(stmt)
                session.commit()
                
                total_inserted += result.rowcount
                print(f"  Chunk {chunk_idx + 1}: Inserted {result.rowcount} rows " +
                      f"(Total: {total_inserted:,}/{total_rows:,})")
            except Exception as e:
                print(f"  ❌ Error in chunk {chunk_idx + 1}: {e}")
                session.rollback()
                raise
    
    print(f"✅ {table_name} insert complete: {total_inserted:,} rows inserted")


def run(db):
    """Run the complete ETL pipeline for this dataset"""
    df = load()
    df = clean(df)
    insert(df, db)


if __name__ == "__main__":
    run_with_session(run)